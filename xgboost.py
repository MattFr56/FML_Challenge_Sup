# -*- coding: utf-8 -*-
"""XGBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WcNggL9SC-oE9uojWM_3Ey1FTcd0AvoS

# **Data Preparation**
"""

#### Import of libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import scipy as sp
from scipy.stats import pearsonr,spearmanr


#### Allow to display all features in a row by using head()
pd.set_option('display.max_columns', 20)

#### Make a list of all paths of the different csv files in "2113" folder.
def file_merging_2113():
    files_list_2113 = []
    for i in range(1, 10):
        files_list_2113.append(
            "/content/XML100_2113_0" + str(i) + ".csv")
    for i in range(10, 28):
        files_list_2113.append(
            "XML100_2113_" + str(i) + ".csv")
    return files_list_2113

def file_merging_2213():
    files_list_2213 = []
    for i in range(1, 10):
        files_list_2213.append(
            "/content/XML100_2213_0" + str(i) + ".csv")
    for i in range(10, 28):
        files_list_2213.append(
            "XML100_2213_" + str(i) + ".csv")
    return files_list_2213

def file_merging_3113():
    files_list_3113 = []
    for i in range(1, 10):
        files_list_3113.append(
            "/content/XML100_3113_0" + str(i) + ".csv")
    for i in range(10, 27):
        files_list_3113.append(
            "XML100_3113_" + str(i) + ".csv")
    return files_list_3113

def file_merging_3213():
    files_list_3213 = []
    for i in range(1, 10):
        files_list_3213.append(
            "/content/XML100_3213_0" + str(i) + ".csv")
    for i in range(10, 27):
        files_list_3213.append(
            "XML100_3213_" + str(i) + ".csv")
    return files_list_3213

#### Loop allowing to read each csv file path and change column names.
ncols = ["instance_name","solution_cost","S1","S2","S3","S4","S5","S6","S7","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"]

def make_df_2113():
    a = []
    for i in range(0, 27):
        a.append(pd.read_csv(file_merging_2113()[i], sep=";",names=ncols))
    return a

def make_df_2213():
    a = []
    for i in range(0, 27):
        a.append(pd.read_csv(file_merging_2213()[i], sep=";",names=ncols))
    return a
    
def make_df_3113():
    a = []
    for i in range(0, 26):
        a.append(pd.read_csv(file_merging_3113()[i], sep=";",names=ncols))
    return a

def make_df_3213():
    a = []
    for i in range(0, 26):
        a.append(pd.read_csv(file_merging_3213()[i], sep=";",names=ncols))
    return a

#### Concatenation of all dataframes to one dataframe
df_2113 = pd.concat(make_df_2113())
df_2213 = pd.concat(make_df_2213())
df_3113 = pd.concat(make_df_3113())
df_3213 = pd.concat(make_df_3213())

"""# **Descriptive Statistics 2113**"""

#### S7 column is an empty column and it will be remove
df_2113.describe()

#### Except instance names, all variables contain integers or floats
df_2113.info()

#### At first sight, df does not get any empty value except S7 (full of 0 values)
df_2113.isnull().sum()

#### remove of "S7" column due to an information reporting lag during genetic algorithm run
df1_2113 = df_2113.drop(["S7"],axis=1)

#### df1 shrinking to only include numerical features
df1_num_2113 = df1_2113.iloc[:,1:]

print(df1_num_2113.head())

#### Data standardization in order to better compare each feature 
from sklearn import preprocessing
stand = preprocessing.StandardScaler().fit(df1_num_2113).transform(df1_num_2113)
df1_nor_2113 = pd.DataFrame(stand, columns=["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"])

df1_nor_2113.shape

#### We observe quite a few outliers in almost all of df1_num_1's features
def boxplot_display(data):
    plt.figure(figsize=(12,7))
    plt.title("Boxplot of normalized data")
    sns.boxplot(data=data)
    plt.show()

boxplot_display(df1_nor_2113)

#### Outliers removing function
def out_remov(data):
  for x in ["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"]:
    q75,q25 = np.percentile(data.loc[:,x],[75,25])
    intr_qr = q75-q25
 
    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)
 
    data.loc[data[x] < min,x] = np.nan
    data.loc[data[x] > max,x] = np.nan
    data.dropna(axis = 0, inplace = True)
  
out_remov(df1_nor_2113)

#### 40 percent of our data has been removed
print("df1_num shape :",df1_num_2113.shape, "\n\n", "df1_num_1 shape :",df1_nor_2113.shape,"\n\n","Percentage of remaining data after removing outliers :",(df1_nor_2113.shape[0]/ df1_num_2113.shape[0])*100)

#### Correlation matrix 

def correlation_matrix():
    df1_corr = df1_nor_2113.corr(method='spearman')
    plt.figure(figsize=(15,10))
    plt.title("Spearmans correlation matrix")
    sns.heatmap(data=df1_corr,annot=True)
    plt.show()

correlation_matrix()

def correlation_matrix():
    df1_corr = df1_nor_2113.corr(method='pearson')
    plt.figure(figsize=(15,10))
    plt.title("Pearsons correlation matrix")
    sns.heatmap(data=df1_corr,annot=True)
    plt.show()

correlation_matrix()

"""# **Descriptive Statistics 2213**"""

df_2213.describe()

df_2213.info()

df_2213.isnull().sum()

#### remove of "S7" column due to an information reporting lag during genetic algorithm run
df1_2213 = df_2213.drop(["S7"],axis=1)

#### df1 shrinking to only include numerical features
df1_num_2213 = df1_2213.iloc[:,1:]

df1_num_2213.head()

#### Data standardization in order to better compare each feature 
from sklearn import preprocessing
stand = preprocessing.StandardScaler().fit(df1_num_2213).transform(df1_num_2213)
df1_nor_2213 = pd.DataFrame(stand, columns=["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"])

df1_nor_2213.shape

#### We observe quite a few outliers in almost all of df1_nor_2213's features
def boxplot_display(data):
    plt.figure(figsize=(12,7))
    plt.title("Boxplot of normalized data")
    sns.boxplot(data=data)
    plt.show()

boxplot_display(df1_nor_2213)

#### Outliers removing function
def out_remov(data):
  for x in ["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"]:
    q75,q25 = np.percentile(data.loc[:,x],[75,25])
    intr_qr = q75-q25
 
    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)
 
    data.loc[data[x] < min,x] = np.nan
    data.loc[data[x] > max,x] = np.nan
    data.dropna(axis = 0, inplace = True)
  
out_remov(df1_nor_2213)

#### 40 percent of our data has been removed
print("df1_num_2213 shape :",df1_num_2213.shape, "\n\n", "df1_nor_2213 shape :",df1_nor_2213.shape,"\n\n","Percentage of remaining data after removing outliers :",(df1_nor_2213.shape[0]/ df1_num_2213.shape[0])*100)

#### Correlation matrix 

def correlation_matrix():
    df1_corr = df1_nor_2213.corr(method='spearman')
    plt.figure(figsize=(15,10))
    plt.title("Spearmans correlation matrix")
    sns.heatmap(data=df1_corr,annot=True)
    plt.show()

correlation_matrix()

"""# **Descriptive Statistics 3113**"""

df_3113.describe()

df_3113.info()

df_3113.isnull().sum()

#### remove of "S7" column due to an information reporting lag during genetic algorithm run
df1_3113 = df_3113.drop(["S7"],axis=1)

#### df1 shrinking to only include numerical features
df1_num_3113 = df1_3113.iloc[:,1:]

df1_num_3113.head()

#### Data standardization in order to better compare each feature 
from sklearn import preprocessing
stand = preprocessing.StandardScaler().fit(df1_num_3113).transform(df1_num_3113)
df1_nor_3113 = pd.DataFrame(stand, columns=["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"])

df1_nor_3113.shape

#### We observe quite a few outliers in almost all of df1_num_1's features
def boxplot_display(data):
    plt.figure(figsize=(12,7))
    plt.title("Boxplot of normalized data")
    sns.boxplot(data=data)
    plt.show()

boxplot_display(df1_nor_3113)

#### Outliers removing function
def out_remov(data):
  for x in ["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"]:
    q75,q25 = np.percentile(data.loc[:,x],[75,25])
    intr_qr = q75-q25
 
    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)
 
    data.loc[data[x] < min,x] = np.nan
    data.loc[data[x] > max,x] = np.nan
    data.dropna(axis = 0, inplace = True)
  
out_remov(df1_nor_3113)

#### About 13 percent of our data has been removed
print("df1_num_3113 shape :",df1_num_3113.shape, "\n\n", "df1_nor_3113 shape :",df1_nor_3113.shape,"\n\n","Percentage of remaining data after removing outliers :",(df1_nor_3113.shape[0]/ df1_num_3113.shape[0])*100)

#### Correlation matrix 

def correlation_matrix():
    df1_corr = df1_nor_2213.corr(method='spearman')
    plt.figure(figsize=(15,10))
    plt.title("Spearmans correlation matrix")
    sns.heatmap(data=df1_corr,annot=True)
    plt.show()

correlation_matrix()

"""# **Descriptive Statistics 3213**"""

df_3213.describe()

df_3213.info()

df_3213.isnull().sum()

#### remove of "S7" column due to an information reporting lag during genetic algorithm run
df1_3213 = df_3213.drop(["S7"],axis=1)

#### df1 shrinking to only include numerical features
df1_num_3213 = df1_3213.iloc[:,1:]

df1_num_3213.head()

#### Data standardization in order to better compare each feature 
from sklearn import preprocessing
stand = preprocessing.StandardScaler().fit(df1_num_3213).transform(df1_num_3213)
df1_nor_3213 = pd.DataFrame(stand, columns=["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"])

df1_nor_3213.shape

#### We observe quite a few outliers in almost all of df1_num_1's features
def boxplot_display(data):
    plt.figure(figsize=(12,7))
    plt.title("Boxplot of normalized data")
    sns.boxplot(data=data)
    plt.show()

boxplot_display(df1_nor_3213)

#### Outliers removing function
def out_remov(data):
  for x in ["solution_cost","S1","S2","S3","S4","S5","S6","S8","S9","S10","S11","S12","S13","S14","S15","S16","S17","S18"]:
    q75,q25 = np.percentile(data.loc[:,x],[75,25])
    intr_qr = q75-q25
 
    max = q75+(1.5*intr_qr)
    min = q25-(1.5*intr_qr)
 
    data.loc[data[x] < min,x] = np.nan
    data.loc[data[x] > max,x] = np.nan
    data.dropna(axis = 0, inplace = True)
  
out_remov(df1_nor_3213)

#### About 13 percent of our data has been removed
print("df1_num_3213 shape :",df1_num_3213.shape, "\n\n", "df1_nor_3113 shape :",df1_nor_3213.shape,"\n\n","Percentage of remaining data after removing outliers :",(df1_nor_3213.shape[0]/ df1_num_3213.shape[0])*100)

"""# **Model 2113**"""

#### Split the data into train and test set
from sklearn.model_selection import train_test_split
X = df1_nor_2113.iloc[:,1:]
y = df1_nor_2113.iloc[:,0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )

#### XGBoost regressor as regression algorithm
#### We have chosen to use RMSE, R2 and MAE as regression metrics to assess our model
#### We have set up model's settings to get the best model ever by paying attention to overfitting 
#### As a matter of fact, we have increased max_depth up to 10 and n_estimators up to 1000 
#### However, we observed that both our model's performances and overfitting have enhanced   


from sklearn.linear_model import LogisticRegression
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from xgboost import XGBRegressor

xgb_clf_1 = xgb.XGBRegressor(objective = 'reg:squarederror',n_estimators=200, max_depth=5, eta=0.01)
xgb_clf_1.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5, verbose=False)
Y_pred = xgb_clf_1.predict(X_test)

from sklearn.metrics import mean_squared_log_error
RMSE = np.sqrt(mean_squared_error(y_test, Y_pred))
R2 = r2_score(y_test, Y_pred)
MAE = mean_absolute_error(y_test, Y_pred)
print("The RMSE is %.5f" % RMSE )
print("The R2 is %.5f" % R2 )
print("The MAE is %.5f" % MAE )
print('Training set score: {:.5f}'.format(xgb_clf_1.score(X_train, y_train)))

print('Testing set score: {:.5f}'.format(xgb_clf_1.score(X_test, y_test)))

#### 
xgb.plot_importance(xgb_clf_1)

"""# **Model 2213**"""

#### Split the data into train and test set
from sklearn.model_selection import train_test_split
X = df1_nor_2213.iloc[:,1:]
y = df1_nor_2213.iloc[:,0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )

#### XGBoost regressor as regression algorithm
#### We have chosen to use RMSE, R2 and MAE as regression metrics to assess our model
#### We have set up model's settings to get the best model ever by paying attention to overfitting 
#### As a matter of fact, we have increased max_depth up to 10 and n_estimators up to 1000 
#### However, we observed that both our model's performances and overfitting have enhanced   


from sklearn.linear_model import LogisticRegression
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from xgboost import XGBRegressor

xgb_clf_1 = xgb.XGBRegressor(objective = 'reg:squarederror',n_estimators=200, max_depth=5, eta=0.01)
xgb_clf_1.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5, verbose=False)
Y_pred = xgb_clf_1.predict(X_test)

from sklearn.metrics import mean_squared_log_error
RMSE = np.sqrt(mean_squared_error(y_test, Y_pred))
R2 = r2_score(y_test, Y_pred)
MAE = mean_absolute_error(y_test, Y_pred)
print("The RMSE is %.5f" % RMSE )
print("The R2 is %.5f" % R2 )
print("The MAE is %.5f" % MAE )
print('Training set score: {:.5f}'.format(xgb_clf_1.score(X_train, y_train)))

print('Testing set score: {:.5f}'.format(xgb_clf_1.score(X_test, y_test)))

#### 
xgb.plot_importance(xgb_clf_1)

"""# **Model 3113**"""

#### Split the data into train and test set
from sklearn.model_selection import train_test_split
X = df1_nor_3113.iloc[:,1:]
y = df1_nor_3113.iloc[:,0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )

#### XGBoost regressor as regression algorithm
#### We have chosen to use RMSE, R2 and MAE as regression metrics to assess our model
#### We have set up model's settings to get the best model ever by paying attention to overfitting 
#### As a matter of fact, we have increased max_depth up to 10 and n_estimators up to 1000 
#### However, we observed that both our model's performances and overfitting have enhanced   


from sklearn.linear_model import LogisticRegression
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from xgboost import XGBRegressor

xgb_clf_1 = xgb.XGBRegressor(objective = 'reg:squarederror',n_estimators=200, max_depth=5, eta=0.01)
xgb_clf_1.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5, verbose=False)
Y_pred = xgb_clf_1.predict(X_test)

from sklearn.metrics import mean_squared_log_error
RMSE = np.sqrt(mean_squared_error(y_test, Y_pred))
R2 = r2_score(y_test, Y_pred)
MAE = mean_absolute_error(y_test, Y_pred)
print("The RMSE is %.5f" % RMSE )
print("The R2 is %.5f" % R2 )
print("The MAE is %.5f" % MAE )
print('Training set score: {:.5f}'.format(xgb_clf_1.score(X_train, y_train)))

print('Testing set score: {:.5f}'.format(xgb_clf_1.score(X_test, y_test)))

#### 
xgb.plot_importance(xgb_clf_1)

"""# **Model 3213**"""

#### Split the data into train and test set
from sklearn.model_selection import train_test_split
X = df1_nor_3213.iloc[:,1:]
y = df1_nor_3213.iloc[:,0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape )

#### XGBoost regressor as regression algorithm
#### We have chosen to use RMSE, R2 and MAE as regression metrics to assess our model
#### We have set up model's settings to get the best model ever by paying attention to overfitting 
#### As a matter of fact, we have increased max_depth up to 10 and n_estimators up to 1000 
#### However, we observed that both our model's performances and overfitting have enhanced   


from sklearn.linear_model import LogisticRegression
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from xgboost import XGBRegressor

xgb_clf_1 = xgb.XGBRegressor(objective = 'reg:squarederror',n_estimators=200, max_depth=5, eta=0.01)
xgb_clf_1.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=5, verbose=False)
Y_pred = xgb_clf_1.predict(X_test)

from sklearn.metrics import mean_squared_log_error
RMSE = np.sqrt(mean_squared_error(y_test, Y_pred))
R2 = r2_score(y_test, Y_pred)
MAE = mean_absolute_error(y_test, Y_pred)
print("The RMSE is %.5f" % RMSE )
print("The R2 is %.5f" % R2 )
print("The MAE is %.5f" % MAE )
print('Training set score: {:.5f}'.format(xgb_clf_1.score(X_train, y_train)))

print('Testing set score: {:.5f}'.format(xgb_clf_1.score(X_test, y_test)))

#### 
xgb.plot_importance(xgb_clf_1)